{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом ноутбуке будет показана возможность использовать предобученные классификаторы для дальнейшего простого и быстрого их дообучения на нужной базе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем это нужно?\n",
    "* Конечная база данных недостаточно представительна для полноценного обучения (сотни или даже десятки картинок)\n",
    "* Имеется предобученная СНС на представительной БД (например, ImageNet)\n",
    "* Нужно быстро получить классификатор на новой (конечной) БД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как это обычно делают в Keras?\n",
    "* Берут предобученную СНС-классификатор\n",
    "* Отрезают все слои, следующие за последним сверточным (Flatten, Dense, Softmax)\n",
    "* Замораживают веса оставшихся слоев (опционально - для ускорения обучения)\n",
    "* Сверху навешивают новую \"голову\" с нужным числом классов и своей функцией потерь\n",
    "* Собирают (компилируют) новую модель и дообучают"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План\n",
    "* Обучим сеть для MNIST\n",
    "* Обучим сеть для Fashion MNIST \n",
    "* Сделаем Transfer Learning для MNIST -> Fashion MNIST\n",
    "* Увидим, что использование этой техники даже в таком игрушечном примере может улучшать качество по сравнению с обычным обучением\n",
    "* Покажем, как делать Transfer Learning для предобученных моделей на Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.layers import Conv2D\n",
    " \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем стандартные интерфейсы для работы с (Fashion) MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем саму базу MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(m_x_train, m_y_train), (m_x_test, m_y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(m_x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(m_x_train.shape, m_x_test.shape, m_y_train.shape, m_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_image = m_x_train[10]\n",
    "plt.imshow(m_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_x_train = m_x_train.astype(np.float32)[..., np.newaxis]/255\n",
    "m_x_test = m_x_test.astype(np.float32)[..., np.newaxis]/255\n",
    "m_y_train = np_utils.to_categorical(m_y_train, num_classes=10)\n",
    "m_y_test = np_utils.to_categorical(m_y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1) (60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(m_x_train.shape, m_x_test.shape, m_y_train.shape, m_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Конструируем простую СНС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alex/Programming/pytenv/py3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "m_input_image = Input(shape=(28, 28, 1))\n",
    "m_conv1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation='relu', data_format='channels_last')(m_input_image)\n",
    "m_conv2 = Conv2D(filters=32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation='relu', data_format='channels_last')(m_conv1)\n",
    "m_flatten = Flatten()(m_conv2)\n",
    "m_dense1 = Dense(128, activation='relu')(m_flatten)\n",
    "m_dense2 = Dense(10, activation='softmax')(m_dense1)\n",
    "m_model = Model(inputs=m_input_image, outputs=m_dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Смотрим краткую инфу по СНС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 211,690\n",
      "Trainable params: 211,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прикручиваем функцию потерь, оптимизатор и набор метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alex/Programming/pytenv/py3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 18s 390us/step - loss: 0.1936 - accuracy: 0.9416 - val_loss: 0.0841 - val_accuracy: 0.9743\n"
     ]
    }
   ],
   "source": [
    "m_history = m_model.fit(m_x_train, m_y_train, validation_split=0.25, batch_size=32, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "#plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "#plt.title('Model accuracy')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend(['Train', 'Val'], loc='upper left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Смотрим качество на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.78000116348267\n"
     ]
    }
   ],
   "source": [
    "m_score = m_model.evaluate(m_x_test, m_y_test, verbose=0)\n",
    "print(100*m_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распечатываем вектор сдвигов для сверток слоя 'conv2d_1' - их 32\n",
    "* Обращаем внимание, как обращаться к слою по имени (который можно посмотреть в model.summary() )\n",
    "* Обращаем внимание, как вытаскивать обучаемые веса слоя через get_weights() )\n",
    "* m_c[0] - ядра сверток, m_c[1] - их сдвиги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.4286174e-03  1.8854599e-03 -2.5786741e-03 -2.1220353e-03\n",
      " -9.6776537e-05  3.1758770e-02 -3.6985474e-03  1.8510371e-03\n",
      " -4.6122575e-04 -1.3554611e-03  1.2931432e-02  3.0735601e-03\n",
      " -2.0843486e-03  6.1635938e-03 -4.3726028e-03  4.9938709e-03\n",
      " -1.3935488e-03  6.2724864e-03  2.3815279e-04  7.9222466e-04\n",
      " -4.3180710e-04  5.1443849e-02  3.0634951e-02  1.4063112e-03\n",
      " -1.8424233e-03  1.9881932e-02 -1.5044523e-03  4.5948618e-05\n",
      " -2.6462940e-03  2.1636998e-03 -2.3130607e-03  3.7237839e-03]\n"
     ]
    }
   ],
   "source": [
    "m_c = m_model.get_layer('conv2d_1').get_weights()\n",
    "print(m_c[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем все то же самое, что и для MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f_x_train, f_y_train), (f_x_test, f_y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_image = f_x_train[10]\n",
    "plt.imshow(f_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_x_train = f_x_train.astype(np.float32)[..., np.newaxis]/255\n",
    "f_x_test = f_x_test.astype(np.float32)[..., np.newaxis]/255\n",
    "f_y_train = np_utils.to_categorical(f_y_train, num_classes=10)\n",
    "f_y_test = np_utils.to_categorical(f_y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_input_image = Input(shape=(28, 28, 1))\n",
    "f_conv1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation='relu', data_format='channels_last')(f_input_image)\n",
    "f_conv2 = Conv2D(filters=32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation='relu', data_format='channels_last')(f_conv1)\n",
    "f_flatten = Flatten()(f_conv2)\n",
    "f_dense1 = Dense(128, activation='relu')(f_flatten)\n",
    "f_dense2 = Dense(10, activation='softmax')(f_dense1)\n",
    "f_model = Model(inputs=f_input_image, outputs=f_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 211,690\n",
      "Trainable params: 211,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 384us/step - loss: 0.4653 - accuracy: 0.8332 - val_loss: 0.3400 - val_accuracy: 0.8770\n"
     ]
    }
   ],
   "source": [
    "f_history = f_model.fit(f_x_train, f_y_train, validation_split=0.25, batch_size=32, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "#plt.plot(fhistory.history['accuracy'])\n",
    "#plt.plot(fhistory.history['val_accuracy'])\n",
    "#plt.title('Model accuracy')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend(['Train', 'Val'], loc='upper left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.87000274658203\n"
     ]
    }
   ],
   "source": [
    "f_score = f_model.evaluate(f_x_test, f_y_test, verbose=0)\n",
    "print(100*f_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00016893  0.00665746  0.00138501  0.08844622  0.05806929  0.01127577\n",
      "  0.11452164  0.08625756  0.02335931 -0.00433461  0.03810786  0.01438208\n",
      "  0.00828008  0.01911275  0.02756644  0.00785673  0.02704236  0.01009052\n",
      "  0.02793615  0.02811772  0.02665378 -0.00480357  0.02036573  0.10128123\n",
      " -0.00155543  0.00641265  0.03861093  0.01480334  0.01535734  0.01004093\n",
      "  0.00611961  0.00298612]\n"
     ]
    }
   ],
   "source": [
    "f_c = f_model.get_layer('conv2d_3').get_weights()\n",
    "print(f_c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST -> Fashion MNIST  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скопируем архитектуру модели MNIST (clone_model) и ее веса (set/get weights), чтобы не повредить изначальную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 211,690\n",
      "Trainable params: 211,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# скопируем модель\n",
    "f2m_model = keras.models.clone_model(m_model)\n",
    "f2m_model.set_weights(m_model.get_weights())\n",
    "f2m_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запомним вектор сдвигов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.4286174e-03  1.8854599e-03 -2.5786741e-03 -2.1220353e-03\n",
      " -9.6776537e-05  3.1758770e-02 -3.6985474e-03  1.8510371e-03\n",
      " -4.6122575e-04 -1.3554611e-03  1.2931432e-02  3.0735601e-03\n",
      " -2.0843486e-03  6.1635938e-03 -4.3726028e-03  4.9938709e-03\n",
      " -1.3935488e-03  6.2724864e-03  2.3815279e-04  7.9222466e-04\n",
      " -4.3180710e-04  5.1443849e-02  3.0634951e-02  1.4063112e-03\n",
      " -1.8424233e-03  1.9881932e-02 -1.5044523e-03  4.5948618e-05\n",
      " -2.6462940e-03  2.1636998e-03 -2.3130607e-03  3.7237839e-03]\n"
     ]
    }
   ],
   "source": [
    "f2m_c = f2m_model.get_layer('conv2d_1').get_weights()\n",
    "print(f2m_c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Убираем последние несверточные слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          9248      \n",
      "=================================================================\n",
      "Total params: 9,568\n",
      "Trainable params: 9,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f2m_model.layers[-3:] = []\n",
    "f2m_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замораживаем сверточные слои из модели MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in f2m_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приделываем новую классификационную голову\n",
    "* Для этого на вход новому слою Flatten подаем выход последнего сверточного слоя из новой модели, который смотрим опять из model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flatten = Flatten()(f2m_model.get_layer('conv2d_2').output)\n",
    "new_dense1 = Dense(128, activation='relu')(new_flatten)\n",
    "new_dense2 = Dense(10, activation='softmax')(new_dense1)\n",
    "\n",
    "new_model = Model(inputs=f2m_model.inputs, outputs=new_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверяем, что после построения новой модели веса сдвигов не поменялись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.4286174e-03  1.8854599e-03 -2.5786741e-03 -2.1220353e-03\n",
      " -9.6776537e-05  3.1758770e-02 -3.6985474e-03  1.8510371e-03\n",
      " -4.6122575e-04 -1.3554611e-03  1.2931432e-02  3.0735601e-03\n",
      " -2.0843486e-03  6.1635938e-03 -4.3726028e-03  4.9938709e-03\n",
      " -1.3935488e-03  6.2724864e-03  2.3815279e-04  7.9222466e-04\n",
      " -4.3180710e-04  5.1443849e-02  3.0634951e-02  1.4063112e-03\n",
      " -1.8424233e-03  1.9881932e-02 -1.5044523e-03  4.5948618e-05\n",
      " -2.6462940e-03  2.1636998e-03 -2.3130607e-03  3.7237839e-03]\n"
     ]
    }
   ],
   "source": [
    "new_c = new_model.get_layer('conv2d_1').get_weights()\n",
    "print(new_c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дообучаем новую модель на Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 9s 202us/step - loss: 0.4294 - accuracy: 0.8469 - val_loss: 0.3488 - val_accuracy: 0.8734\n"
     ]
    }
   ],
   "source": [
    "new_history = new_model.fit(f_x_train, f_y_train, validation_split=0.25, batch_size=32, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На всякий случай еще раз проверим, что веса сдвигов были заморожены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.4286174e-03  1.8854599e-03 -2.5786741e-03 -2.1220353e-03\n",
      " -9.6776537e-05  3.1758770e-02 -3.6985474e-03  1.8510371e-03\n",
      " -4.6122575e-04 -1.3554611e-03  1.2931432e-02  3.0735601e-03\n",
      " -2.0843486e-03  6.1635938e-03 -4.3726028e-03  4.9938709e-03\n",
      " -1.3935488e-03  6.2724864e-03  2.3815279e-04  7.9222466e-04\n",
      " -4.3180710e-04  5.1443849e-02  3.0634951e-02  1.4063112e-03\n",
      " -1.8424233e-03  1.9881932e-02 -1.5044523e-03  4.5948618e-05\n",
      " -2.6462940e-03  2.1636998e-03 -2.3130607e-03  3.7237839e-03]\n"
     ]
    }
   ],
   "source": [
    "new_c = new_model.get_layer('conv2d_1').get_weights()\n",
    "print(new_c[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравним качество дообученной модели с исходной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion model acc = 86.87000274658203 MNIST->Fashion model acc = 86.87999844551086\n"
     ]
    }
   ],
   "source": [
    "new_score = new_model.evaluate(f_x_test, f_y_test, verbose=0)\n",
    "print('Fashion model acc =', 100*f_score[1], 'MNIST->Fashion model acc =', 100*new_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с предобученными моделями в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера возьмем легкую MobileNet, все же доступные предобученные модели можно посмотреть здесь - https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "mobile_net = MobileNet(input_shape=(224,224,3), include_top=False, weights='imagenet', classes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Важные параметры\n",
    "* input_shape - (224,224,3) или (3, 224, 224) в зависимости от того, когда у вас идут каналы изображения (channels_last или channels_first)\n",
    "* include_top=False - самый важный параметр, говорит о том, что нужно отрезать всю голову СНС после последней свертки\n",
    "* weights='imagenet' - подгружаем веса СНС, обученной на ImageNet (classes не задаем)\n",
    "* Если же хотим просто использовать архитектуру без весов, то weights=None и classes=нужное_количество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_1.00_224\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobile_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заморозка слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in mobile_net.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приделывание новой головы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mobile_net.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(111, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Завершение создания новой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the final model \n",
    "model_final = Model(inputs = mobile_net.inputs, outputs = predictions)\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ну и дальше обучение model_final ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
